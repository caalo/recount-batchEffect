---
title: "SVA Simulation"
author: "Christopher Lo"
date: "9/6/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=F, message=F, warning=F}
library(sva)
library(tidyverse)
library(DESeq2)
theme_set(theme_bw())
```


Simulation studies inspired from
*"A general framework for multiple testing dependence"* (Leek et al. 2008)

# Simulation Set-Up, one single experiment

We generate $X$ from the following model: $$X = BS + \Gamma G + U$$ 

We have $m = 1000$ genes (tests), $n = 20$ samples, and $r = 2$ latent variables.

Sampling noise: $U_{m,n} \sim N(0, 1)$.

The design matrix $S$ is 10 cases and 10 controls: $S_{1, n} = 1$ for $n=1:20$. Then, $S_{2, n} = 0$ for $n = 1:10$, $S_{2, n} = 1$ for $n = 11:20$.

Control effect for all genes: $b_{m,1} \sim N(0, 1), m=1:1000$

Case effect for DE genes $m=1:300$: $b_{m, 2} \sim N(3, 1)$

Case effect for Non-DE genes $m=301:1000$: $b_{m, 2} \sim N(0, 2)$

Latent design matrix (kernel) $G$: $G_{r, n} \sim Bernoulli(.2), n=1:10$. $G_{r, n} \sim Bernoulli(.8), n=11:20$, where $r = 1, 2$. (This ensures correlation between the two design matrices. The stronger the correlation, the less signal we will see, because the latent effects lead to FPs and FNs if the design matricies are similar.)

Latent effect 1: $\Gamma_{m, 1} \sim N(0, 1), m=1:300$,  $\Gamma_{m, 1} \sim N(1, 2), m=301:1000$. (Positive signal overlaps with Non-DE genes, will lead to FPs if not corrected)

Latent effect 2: $\Gamma_{m, 2} \sim N(-1, 2), m=1:300$, $\Gamma_{m, 2} \sim N(0, 1), m=301:1000$. (Negative signal overlaps with DE genes, will lead to FNs if not corrected)

Therefore, for every gene, whether it is DE or not, it will be affected by one of the two latent variables 

**To ask/consider: **

-   When there's no effect, should we use $N(0, 1)$, or should we just use $0$?

-   Currently we have negative expression due to Latent effect 2.

-   Should we normalize before running analysis?


```{r echo=F}
set.seed(2023)
m = 1000 #number of genes (tests)
n = 20 #number of samples
r = 2

U = rnorm(n, mean = 0, sd = 1)

b1 = rnorm(m, mean = 0, sd = 1) #intercept
b2 = mapply(mu = c(rep(3, 300), rep(0, m - 300)),
            sigma = c(rep(1, 300), rep(1, m - 300)),
             function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
B = cbind(b1, b2)
S = matrix(c(rep(1, n), rep(0, 10), rep(1, 10)), byrow = T, ncol = n)

gamma1 = mapply(mu = c(rep(0, 300), rep(1.5, m - 300)),
                sigma = c(rep(1, 300), rep(2, m - 300)),
                function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
gamma2 = mapply(mu = c(rep(-1.5, 300), rep(0, m - 300)),
                sigma = c(rep(2, 300), rep(1, m - 300)),
                function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
Gamma = cbind(gamma1, gamma2)
G = mapply(p = c(rep(.2, 10), rep(.8, 10)), #no need for intercept for kernel G.
           function(p) rbernoulli(2, p))
# G = mapply(mu = c(rep(0, 10), rep(.3, 10)), #no need for intercept for kernel G. 
#            sigma = c(rep(1, 20)),
#            function(mu, sigma) rnorm(2, mean = mu, sd = sigma))

X = B %*% S + Gamma %*% G + U
```

# Estimate the number of SVs:

```{r echo=T, warning=F, message=F}
n.sv = num.sv(X, t(S), method = "be")
cat("Number of SVs: ", n.sv, "\n")

pca = prcomp(t(X))
variance = pca$sdev^2 / sum(pca$sdev^2)
qplot(c(1:length(variance)), variance) + geom_line() + geom_point() + 
  geom_hline(yintercept=1/ncol(X), linetype = "dashed") +
  xlab("Principal Component") + ylab("Variance Explained") + ggtitle(paste0("Number of SVs: ", n.sv)) + ylim(0, 1)
```

# Estimate SVs, primary variable coefficients, and SV coefficients

-   Are latent variables are spanned by the estimated SVs?

-   Are the estimated coefficients similar to true coefficients?

-   Is the null p-value distribution uniform?

-   Do the ranks of top genes match?


```{r echo=T, warning=F, message=F}

nullMod = t(S)[, 1]
svobj = sva(X, t(S), nullMod, n.sv = n.sv)

#visually look at predicted SVs. 
qplot(as.numeric(G[1 ,]), svobj$sv[, 1], xlab = "True latent variable 1", ylab = "Est. SV1")
qplot(as.numeric(G[2 ,]), svobj$sv[, 2], xlab = "True latent variable 2", ylab = "Est. SV2")

nullmodsv = cbind(nullMod, svobj$sv)
modsv = cbind(t(S), svobj$sv)
#run full regression.
fitsv = lm.fit(modsv, t(X))

#visually look at predicted coefficients
plot_df = data.frame(b1 = B[, 1], 
                     b2 = B[, 2], 
                     b1_hat = fitsv$coefficients[1 ,],
                     b2_hat = fitsv$coefficients[2 ,],
                     b2_labels = c(rep("alt", 300), rep("null", m - 300)),
                     gamma1 = Gamma[, 1],
                     gamma1_hat = fitsv$coefficients[3 ,],
                     gamma1_labels = c(rep("alt", 300), rep("null", m - 300)),
                     gamma2 = Gamma[, 2],
                     gamma2_hat = fitsv$coefficients[4 ,],
                     gamma2_labels = c(rep("alt", 300), rep("null", m - 300)))

ggplot(plot_df, aes(b1, b1_hat)) + geom_point() + labs(x = "True b_m1", y = "Est. b_m1")
ggplot(plot_df, aes(b2, b2_hat)) + geom_point() + facet_wrap(~b2_labels) + labs(x = "True b_m2", y = "Est. b_m2")
ggplot(plot_df, aes(gamma1, gamma1_hat)) + geom_point() + facet_wrap(~gamma1_labels) + labs(x = "True gamma_m1", y = "Est. gamma_m1")
ggplot(plot_df, aes(gamma2, gamma2_hat)) + geom_point() + facet_wrap(~gamma2_labels) + labs(x = "True gamma_m2", y = "Est. gamma_m2")
```

Not sure what's going on here yet regarding p-values and ranking.

-   Why does `f.pvalue()` function look very different than `anova()` function?

-   Why are we getting no p values < .05?

```{r echo=F}
#just compute p-value of b_12 = 0 using F statistics. 
pValuesSv = f.pvalue(X, modsv, nullmodsv)
pValues = f.pvalue(X, t(S), as.matrix(nullMod))

#double check with existing function:
pValuesSV2 = rep(NA, 1000)
fstat = rep(NA, 1000)
for(i in 1:1000) { 
  dat = data.frame(x = X[i ,], pv = S[2 ,], sv1 = svobj$sv[, 1], sv2 = svobj$sv[, 2])
  dat_nullmod = lm(x ~ sv1 + sv2, dat)
  dat_mod = lm(x ~ pv + sv1 + sv2, dat)
  an = anova(dat_nullmod, dat_mod)
  pValuesSV2[i] = an$`Pr(>F)`[2]
  fstat[i] = an$F[2]
}


plot(1:length(pValues[301:1000])/(length(pValues[301:1000])+1),sort(pValues[301:1000]), main = "No SV control, No signal", xlab = "P-values", ylab = "Uniform(0, 1)", xlim = c(0, 1), ylim = c(0, 1))
abline(a = 0, b = 1)

plot(1:length(pValuesSv[301:1000])/(length(pValuesSv[301:1000])+1),sort(pValuesSv[301:1000]), main = "SV controlled, No signal", xlab = "P-values", ylab = "Uniform(0, 1)", xlim = c(0, 1), ylim = c(0, 1))
abline(a = 0, b = 1)

plot(1:length(pValues[1:300])/(length(pValues[1:300])+1),sort(pValues[1:300]), main = "No SV control, Signal", xlab = "P-values", ylab = "Uniform(0, 1)", xlim = c(0, 1), ylim = c(0, 1))
abline(a = 0, b = 1)

plot(1:length(pValuesSv[1:300])/(length(pValuesSv[1:300])+1),sort(pValuesSv[1:300]), main = "SV controlled, Signal", xlab = "P-values", ylab = "Uniform(0, 1)", xlim = c(0, 1), ylim = c(0, 1))
abline(a = 0, b = 1)



# 
# print(ks.test(qValuesSV2[301:1000],"punif",0,1))
# 
# print(table(qValuesSV2[301:1000] < .05))
# print(table(qValuesSV2[1:300] < .05))


FStat_order = order(fstat, decreasing = T)
true_order = order(abs(b2), decreasing = T)

qplot(FStat_order, true_order) + geom_smooth()
```


## "Knobs to turn" in estimating the number of SVs

$\Gamma_{m, 1}$: If strong effect relative to $b_{m, 1}$ (fixed), then this will generate noise on control samples, leading to false positives. 

$\Gamma_{m, 2}$: If strong effect relative to $b_{m, 2}$ (fixed), then this will generate noise on case samples, leading to false negatives.

Our certainty of $\Gamma$ to effect case or control samples depends on "the percentage of row space of $S$ explained by $G$". We appropx that by looking at $cor(G_r, S_2), r = 1, 2$. We probably can fix this value for now.



### Knob Speculation, within one experiment

| $\Gamma_{m, 1}$ 	| $\Gamma_{m, 2}$ 	| $cor(G_r, S_2)$ 	| **DE**           	| **Scree plot**     	|
|-----------------	|-----------------	|------------------	|------------------	|--------------------	|
| strong          	| weak            	| strong           	| more FPs         	| more even PCA      	|
| weak            	| strong          	| strong           	| more FNs         	| more even PCA      	|
| weak            	| weak            	| strong           	| neutral          	| more dominated PCA 	|
| strong          	| strong          	| strong           	| more FPs and FNs 	| more even PCA      	|


# Simulation with multiple experiments

$$X = B_1 S_1 + \Gamma_1 G_1 + \alpha(B_2 S_2 + \Gamma_2 G_2) +  U$$ 

where $B_i$ and $\Gamma_i$ are the same shape and distributions of $B$ and $\Gamma$ as before.

$S_1$ is the design matrix of the primary variables of the first experiment, elongated to $0$s for the second experiment. 
etc.

We keep $cor(G_{ir}, S_{i2})$ at the same strength, and vary the SV effect $\Gamma_{m, 1}$, $\Gamma_{m, 2}$, and the study effect, $\alpha$

```{r echo=F}

sum_of_span_residue = function(X, Y) {
  X = as.matrix(X)
  Y = as.matrix(Y)
  stopifnot(nrow(X) == nrow(Y))
  mod1 = lm.fit(X, Y)
  mod1_res = norm(mod1$fitted.values - Y, type = "F") / (nrow(Y) * ncol(Y))
  mod2 = lm.fit(Y, X)
  mod2_res = norm(mod2$fitted.values - X, type = "F") / (nrow(X) * ncol(X))
  return(mod1_res + mod2_res)
}

myPairs = function(df1, df2, bottom = "", left = "") {
  stopifnot(nrow(df1) == nrow(df2))
  plots = list()
  k = 1
  for(i in 1:ncol(df1)) {i
    for (j in 1:ncol(df2)) {
      correlation = round(cor(df1[, i], df2[, j]), 2)
      df = data.frame(x = df1[, i], y = df2[, j])
      plots[[k]] = ggplot(df, aes(x, y)) + geom_point(size = .8) + ggtitle(correlation) +
        theme(axis.text.x=element_blank(),
              axis.text.y=element_blank(),
              axis.ticks=element_blank(),
              legend.position="none",
              axis.title.x=element_blank(),
              axis.title.y=element_blank())
      k = k + 1
    }
  }
  top = paste0("SV Spanning Score: ", round(sum_of_span_residue(df1, df2), 4))
  print(marrangeGrob(plots, nrow = ncol(df1), ncol = ncol(df2), top = top, bottom = bottom, left = left))
}
```


```{r, echo=F, message=F, warning=F, results='hide'}
set.seed(2023)
m = 1000 #number of genes (tests)
n = 20 #number of samples
r = 2 #number of latent variables per studies
s = 2 #number of studies

DE_effect = 3
simulation = expand.grid(alpha = c(.75, 1.5, seq(3, 21, 3)), #effect of the second experiment relative to the first. 
                         gamma = c(.75, 1.5, seq(3, 21, 3))) 
simulation$n.sv = NA
simulation$num_PC_signif = NA
simulation$var_explained_PC1 = NA
simulation$sum_of_span_residue1_vs_truth = NA
simulation$sum_of_span_residue2_vs_truth = NA

forced_analysis = expand.grid(alpha = c(.75, 1.5, seq(3, 21, 3)), 
                              gamma = c(.75, 1.5, seq(3, 21, 3)),
                              forced_SV = 1:6) 
forced_analysis$sum_of_span_residue_joint_forced_vs_truth = NA


for(i in 1:nrow(simulation)) {
  alpha = simulation$alpha[i]
  gamma = simulation$gamma[i]
  #cat(alpha, "\t", gamma, "\n")
  
  #generate data
  U = rnorm(n * s, mean = 0, sd = 1)
  
  b11 = rnorm(m, mean = 0, sd = 1) #intercept
  b12 = mapply(mu = c(rep(DE_effect, 300), rep(0, m - 300)),
               sigma = c(rep(1, 300), rep(1, m - 300)),
               function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
  B1 = cbind(b11, b12)
  b21 = rnorm(m, mean = 0, sd = 1)
  b22 = mapply(mu = c(rep(DE_effect, 300), rep(0, m - 300)),
               sigma = c(rep(1, 300), rep(1, m - 300)),
               function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
  B2 = cbind(b21, b22)
  
  
  S1 = matrix(c(rep(1, n), rep(0, n), rep(0, n/2), rep(1, n/2), rep(0, n)), byrow = T, ncol = s*n)
  S2 = matrix(c(rep(0, n), rep(1, n), rep(0, n), rep(0, n/2), rep(1, n/2)), byrow = T, ncol = s*n)
  
  gamma11 = mapply(mu = c(rep(0, 300), rep(gamma, m - 300)),
               sigma = c(rep(1, 300), rep(1, m - 300)),
               function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
  gamma12 = mapply(mu = c(rep(-gamma, 300), rep(0, m - 300)),
               sigma = c(rep(1, 300), rep(1, m - 300)),
               function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
  Gamma1 = cbind(gamma11, gamma12)
  gamma21 = mapply(mu = c(rep(0, 300), rep(gamma, m - 300)),
               sigma = c(rep(1, 300), rep(1, m - 300)),
               function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
  gamma22 = mapply(mu = c(rep(-gamma, 300), rep(0, m - 300)),
               sigma = c(rep(1, 300), rep(1, m - 300)),
               function(mu, sigma) rnorm(1, mean = mu, sd = sigma))
  Gamma2 = cbind(gamma21, gamma22)
  
  G_zeros = matrix(rep(0, n*2), byrow = T, ncol = n)
  G1 = cbind(mapply(p = c(rep(.2, 10), rep(.8, 10)), 
             function(p) rbinom(2, 1, p)), G_zeros)
  G2 = cbind(G_zeros, mapply(p = c(rep(.2, 10), rep(.8, 10)),
             function(p) rbinom(2, 1, p)))
  
  X1 = B1 %*% S1 + Gamma1 %*% G1 + U
  X1 = X1[, 1:n]
  X2 = alpha * (B2 %*% S2 + Gamma2 %*% G2) + U
  X2 = X2[, (n+1):(2*n)]
  X = B1 %*% S1 + Gamma1 %*% G1 + alpha * (B2 %*% S2 + Gamma2 %*% G2) + U
  
  #Design matricies for inference
  Study_indicator = c(rep(0, n), rep(1, n))
  S_merged = cbind(S1[, 1:n], S2[, (n+1):(2*n)])
  S_merged_withStudy = rbind(S_merged, Study_indicator)
  G_merged = cbind(G1[, 1:n], G2[, (n+1):(2*n)])

  #Normalize
  # X[X < 0] = 0
  # dds <- DESeqDataSetFromMatrix(countData = X, colData = t(S_merged), design = ~t(S_merged)[, 2])
  # dds <- estimateSizeFactors(dds)
  # geneCountsNormalized <- counts(dds, normalized = TRUE)
  
  #Inference
  #PCA
  pca = prcomp(t(X))
  variance = pca$sdev^2 / sum(pca$sdev^2)
  simulation$num_PC_signif[i] = length(which(variance > 1/ncol(X)))
  simulation$var_explained_PC1[i] = variance[1]
  #SV
  #single study
  x1_n.sv = num.sv(X1, t(S1[, 1:n]), method = "be")
  x1_svobj = sva(X1, t(S1[, 1:n]), t(S1[, 1:n])[, 1], n.sv = x1_n.sv)
  x2_n.sv = num.sv(X2, t(S2[, (n+1):(2*n)]), method = "be")
  x2_svobj = sva(X2, t(S2[, (n+1):(2*n)]), t(S2[, (n+1):(2*n)])[, 1], n.sv = x2_n.sv)
  #Individual study spanning residue
  simulation$sum_of_span_residue1_vs_truth[i] = sum_of_span_residue(x1_svobj$sv, t(G1)[1:n ,])
  simulation$sum_of_span_residue2_vs_truth[i] = sum_of_span_residue(x2_svobj$sv, t(G2)[(n+1):(2*n) ,])
  #joint study
  simulation$n.sv[i] = num.sv(X, t(S_merged_withStudy), method = "be")
  #joint_svobj = sva(X, t(S_merged_withStudy), t(S_merged_withStudy)[, c(1, 3)], n.sv =  simulation$n.sv[i])
  #Joint study spanning residue
  idx = which(forced_analysis$alpha == simulation$alpha[i] & forced_analysis$gamma == simulation$gamma[i])
  for(j in idx) {
    cat("\n", forced_analysis$forced_SV[j], "\n")
    joint_svobj_forced = tryCatch({
        sva(X, t(S_merged_withStudy), t(S_merged_withStudy)[, c(1, 3)], n.sv = forced_analysis$forced_SV[j])
    }, error = function(e) {
        joint_svobj_forced = NA
    }, finally = {})
    if(!is.na(joint_svobj_forced)) {
      forced_analysis$sum_of_span_residue_joint_forced_vs_truth[j] = sum_of_span_residue(joint_svobj_forced$sv, t(G_merged))
    }
  }

  # simulation$sum_of_span_residue1_vs_joint[i] = sum_of_span_residue(x1_svobj$sv, joint_svobj$sv[1:n, ])
  # simulation$sum_of_span_residue2_vs_joint[i] = sum_of_span_residue(x2_svobj$sv, joint_svobj$sv[(n+1):(2*n), ])
  # simulation$sum_of_span_residue1_vs_joint_forced[i] = sum_of_span_residue(x1_svobj$sv, joint_svobj_forced$sv[1:n ,])
  # simulation$sum_of_span_residue2_vs_joint_forced[i] = sum_of_span_residue(x2_svobj$sv, joint_svobj_forced$sv[(n+1):(2*n) ,])
  # 
  
  # myPairs(x1_svobj$sv, joint_svobj$sv[1:n ,], left = "Study 1", bottom = "Study 1 + 2")
  # myPairs(x2_svobj$sv, joint_svobj$sv[(n+1):(2*n) ,], left = "Study 2", bottom = "Study 1 + 2")
  # myPairs(x1_svobj$sv, joint_svobj_forced$sv[1:n ,], left = "Study 1", bottom = "Study 1 + 2 forced")
  # myPairs(x2_svobj$sv, joint_svobj_forced$sv[(n+1):(2*n) ,], left = "Study 2", bottom = "Study 1 + 2 forced")
  # 
}

```



```{r echo=F}
ggplot(simulation, aes(x = num_PC_signif, y = n.sv)) + geom_point() + ggtitle("Expected SV: 4")

ggplot(simulation, aes(x = as.factor(alpha/DE_effect), y = as.factor(gamma/DE_effect), fill =  n.sv)) + geom_tile() + 
  labs(x = "Noise:Signal of study effect = alpha/DE_effect",
       y = "Noise:Signal of latent variables = gamma/DE_effect") +
   ggtitle("Expected SV: 4")

ggplot(simulation, aes(x =  as.factor(alpha/DE_effect), y = as.factor(gamma/DE_effect), fill = var_explained_PC1)) + geom_tile() + 
  labs(x = "Noise:Signal of study effect = alpha/DE_effect",
       y = "Noise:Signal of latent variables = gamma/DE_effect",
       fill = "PC1_var")

ggplot(simulation, aes(x =  as.factor(alpha/DE_effect), y = as.factor(gamma/DE_effect), fill = sum_of_span_residue1_vs_truth)) + geom_tile() + 
  labs(x = "Noise:Signal of study effect = alpha/DE_effect",
       y = "Noise:Signal of latent variables = gamma/DE_effect",
       fill = "Study 1\nSpan residule") +
  scale_fill_continuous(limits = c(0, .2))

ggplot(simulation, aes(x =  as.factor(alpha/DE_effect), y = as.factor(gamma/DE_effect), fill = sum_of_span_residue2_vs_truth)) + geom_tile() + 
  labs(x = "Noise:Signal of study effect = alpha/DE_effect",
       y = "Noise:Signal of latent variables = gamma/DE_effect",
      fill = "Study 2\nSpan residule") +
  scale_fill_continuous(limits = c(0, .2))


# 
# ggplot(simulation, aes(x =  as.factor(alpha/DE_effect), y = as.factor(gamma/DE_effect), fill = sum_of_span_residue1_vs_joint)) + geom_tile() + 
#   labs(x = "Noise:Signal of study effect = alpha/DE_effect",
#        y = "Noise:Signal of latent variables = gamma/DE_effect") +
#   scale_fill_continuous(limits = c(0, .1))
# 
# ggplot(simulation, aes(x =  as.factor(alpha/DE_effect), y = as.factor(gamma/DE_effect), fill = sum_of_span_residue1_vs_joint_forced)) + geom_tile() + 
#   labs(x = "Noise:Signal of study effect = alpha/DE_effect",
#        y = "Noise:Signal of latent variables = gamma/DE_effect") +
#   scale_fill_continuous(limits = c(0, .1))
# 
# ggplot(simulation, aes(x =  as.factor(alpha/DE_effect), y = as.factor(gamma/DE_effect), fill = sum_of_span_residue2_vs_joint)) + geom_tile() + 
#   labs(x = "Noise:Signal of study effect = alpha/DE_effect",
#        y = "Noise:Signal of latent variables = gamma/DE_effect") +
#   scale_fill_continuous(limits = c(0, .1))
# 
# ggplot(simulation, aes(x =  as.factor(alpha/DE_effect), y = as.factor(gamma/DE_effect), fill = sum_of_span_residue2_vs_joint_forced)) + geom_tile() + 
#   labs(x = "Noise:Signal of study effect = alpha/DE_effect",
#        y = "Noise:Signal of latent variables = gamma/DE_effect") +
#   scale_fill_continuous(limits = c(0, .1))
   

```


```{r echo=F, fig.width=14, fig.height=18}
ggplot(forced_analysis, aes(forced_SV, sum_of_span_residue_joint_forced_vs_truth)) + geom_point() + geom_line() + facet_grid(alpha/DE_effect ~ gamma/DE_effect)
```


